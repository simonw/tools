{
  "description": "Explore OpenAI's prompt caching feature by testing different prompt structures and observing cache hit rates across multiple requests. This interactive playground lets you compose system instructions, document context, and user questions, then send them to the Chat Completions or Responses API to track how cached tokens reduce costs and improve performance. Load pre-built scenarios to compare caching behavior\u2014such as sending identical instructions with different questions to see cache hits, or swapping documents while keeping instructions static to measure prefix reuse.",
  "commit": "1d6baf40110241775a4260d330a28cd23cb5cfd9"
}
